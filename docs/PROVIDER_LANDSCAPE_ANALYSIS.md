# Provider Landscape Analysis & Roadmap

**Document Version**: 1.0
**Last Updated**: 2026-01-14
**Purpose**: Identify all required providers, their implementation status, and roadmap

---

## Executive Summary

Acode currently supports **2 local providers** with complete lifecycle management. **3-5 cloud providers** are planned for the Cloud Burst phase. This document provides a comprehensive inventory, gap analysis, and roadmap.

### Current Status

| Category | Status | Providers |
|----------|--------|-----------|
| **Local (Fully Implemented)** | âœ… Complete | Ollama, vLLM |
| **Local (Planned)** | ğŸ”„ Designed | Llama.cpp, LM Studio |
| **Cloud (Planned)** | ğŸ’¡ Future | AWS Bedrock, Azure OpenAI, Google Vertex AI |
| **Intentionally Excluded** | âŒ Deferred | Anthropic API (violates LocalOnly philosophy) |

---

## Provider Matrix

### Dimension 1: Execution Location

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WHERE DOES INFERENCE RUN?                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LOCAL (User's machine)                              â”‚
â”‚   â”œâ”€ Ollama (Task 005) âœ…                           â”‚
â”‚   â”œâ”€ vLLM (Task 006) âœ…                             â”‚
â”‚   â”œâ”€ Llama.cpp (Future) ğŸ’¡                          â”‚
â”‚   â””â”€ LM Studio (Future) ğŸ’¡                          â”‚
â”‚                                                     â”‚
â”‚ CLOUD (Managed service)                             â”‚
â”‚   â”œâ”€ AWS Bedrock (Task 029+) ğŸ”„                     â”‚
â”‚   â”œâ”€ Azure OpenAI (Task 029+) ğŸ”„                    â”‚
â”‚   â”œâ”€ Google Vertex AI (Task 029+) ğŸ”„                â”‚
â”‚   â””â”€ Anthropic (EXCLUDED) âŒ                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dimension 2: Lifecycle Management Need

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HOW DOES ACODE MANAGE THE SERVICE?                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MANAGED (Acode starts/stops/monitors)                â”‚
â”‚   â”œâ”€ Ollama (Task 005d) âœ…                           â”‚
â”‚   â”œâ”€ vLLM (Task 006d) âœ…                             â”‚
â”‚   â”œâ”€ Llama.cpp (Future - optional) ğŸ’¡                â”‚
â”‚   â””â”€ LM Studio (Future - optional) ğŸ’¡                â”‚
â”‚                                                      â”‚
â”‚ UNMANAGED (User manages externally)                  â”‚
â”‚   â”œâ”€ AWS Bedrock (cloud-hosted, no startup) ğŸ”„      â”‚
â”‚   â”œâ”€ Azure OpenAI (cloud-hosted) ğŸ”„                 â”‚
â”‚   â””â”€ Google Vertex AI (cloud-hosted) ğŸ”„             â”‚
â”‚                                                      â”‚
â”‚ HYBRID (Both options)                                â”‚
â”‚   â””â”€ Any local provider (Monitored or Managed) ğŸ”„    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dimension 3: Operating Mode Compatibility

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WHICH OPERATING MODES SUPPORT THIS PROVIDER?           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LocalOnly Mode (no external network)                   â”‚
â”‚   â”œâ”€ Ollama âœ…                                        â”‚
â”‚   â”œâ”€ vLLM âœ…                                          â”‚
â”‚   â”œâ”€ Llama.cpp ğŸ’¡                                     â”‚
â”‚   â””â”€ LM Studio ğŸ’¡                                     â”‚
â”‚   âŒ NO Cloud providers                               â”‚
â”‚                                                        â”‚
â”‚ Burst Mode (cloud allowed, but NOT external LLM APIs) â”‚
â”‚   â”œâ”€ Ollama âœ…                                        â”‚
â”‚   â”œâ”€ vLLM âœ…                                          â”‚
â”‚   â”œâ”€ AWS Bedrock ğŸ”„                                   â”‚
â”‚   â”œâ”€ Azure OpenAI ğŸ”„                                  â”‚
â”‚   â””â”€ Google Vertex AI ğŸ”„                              â”‚
â”‚   âŒ Anthropic (violates "no external LLM")          â”‚
â”‚                                                        â”‚
â”‚ Airgapped Mode (zero network)                         â”‚
â”‚   â”œâ”€ Ollama âœ… (pre-staged models only)              â”‚
â”‚   â”œâ”€ vLLM âœ… (pre-staged models only)                â”‚
â”‚   â”œâ”€ Llama.cpp ğŸ’¡ (pre-staged models only)           â”‚
â”‚   â””â”€ LM Studio ğŸ’¡ (pre-staged models only)           â”‚
â”‚   âŒ NO Cloud providers                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Current Providers (Fully Implemented âœ…)

### 1. Ollama

| Aspect | Details |
|--------|---------|
| **Status** | âœ… Complete (Tasks 005, 005a, 005c, 005d) |
| **Website** | https://ollama.ai |
| **Type** | Local service, CPU/GPU |
| **Models Supported** | 100+ (Llama, Mistral, Neural Chat, etc.) |
| **Inference API** | HTTP REST API (custom format) |
| **Task Assignments** | |
| - Core Adapter | Task 005a - Request/Response + Streaming |
| - Smoke Tests | Task 005c - Setup Docs + Test Scripts |
| - Lifecycle | Task 005d - Auto-start, Health, Recovery |
| **Lifecycle Management** | âœ… Full implementation in 005d |
| **Operating Modes** | LocalOnly, Burst, Airgapped |
| **GPU Support** | âœ… NVIDIA, AMD (optional) |
| **Model Pull** | âœ… Automatic via lifecycle manager |
| **CLI Commands** | `acode providers start/stop/status ollama` |
| **Configuration** | `.agent/config.yml` â†’ `providers.ollama.lifecycle` |

**Strengths**:
- Simple installation (single binary)
- Works CPU-only (great for testing)
- No GPU drivers required for basic use
- Excellent documentation

**Limitations**:
- Slower than vLLM
- CPU-only quite slow
- Limited to Ollama's curated models

---

### 2. vLLM

| Aspect | Details |
|--------|---------|
| **Status** | âœ… Complete (Tasks 006, 006a, 006c, 006d) |
| **Website** | https://github.com/lm-sys/vllm |
| **Type** | Local service, GPU-optimized |
| **Models Supported** | 1000+ Huggingface models |
| **Inference API** | OpenAI-compatible REST API |
| **Task Assignments** | |
| - Core Adapter | Task 006a - Serving + Client Adapter |
| - Health Check | Task 006c - Load/Health Endpoints |
| - Lifecycle | Task 006d - Auto-start, GPU Mgmt, Recovery |
| **Lifecycle Management** | âœ… Full implementation in 006d |
| **Operating Modes** | LocalOnly, Burst, Airgapped |
| **GPU Support** | âœ… NVIDIA (primary), AMD (secondary) |
| **Model Pull** | âœ… Automatic via Huggingface |
| **CLI Commands** | `acode providers start/stop/status vllm` |
| **Configuration** | `.agent/config.yml` â†’ `providers.vllm.lifecycle` |

**Strengths**:
- 10x faster than Ollama (GPU-optimized)
- Access to latest Huggingface models
- Production-ready inference serving
- Structured outputs support

**Limitations**:
- GPU required for reasonable performance
- More complex setup (Python + CUDA)
- Higher memory overhead

---

## Planned Providers (Cloud Burst Phase - Task 029+)

### 3. AWS Bedrock

| Aspect | Details |
|--------|---------|
| **Status** | ğŸ”„ Planned (Task 029+) |
| **Website** | https://aws.amazon.com/bedrock |
| **Type** | Managed cloud service |
| **Models** | Claude, Llama 2, Mistral, etc. |
| **API** | AWS SDK (boto3 + Bedrock Runtime) |
| **Lifecycle** | No process management (cloud-hosted) |
| **Operating Modes** | Burst mode only |
| **Cost Model** | Pay-per-1K tokens |
| **Task Assignment** | Task 029+ (Cloud Burst Orchestrators) |
| **Estimated Tasks** | 1-2 subtasks (adapter + cost tracking) |
| **Priority** | HIGH (AWS most popular for burst) |

**Rationale for Selection**:
- âœ… No "external LLM API" violation (Bedrock hosts AWS models like Llama)
- âœ… Enterprise support
- âœ… No rate limiting concerns (AWS managed)
- âœ… Cost tracking/quota support

**Implementation Needs**:
1. CloudServiceOrchestrator wrapper
2. BedrockProvider adapter
3. Cost tracking integration
4. Quota enforcement

---

### 4. Azure OpenAI

| Aspect | Details |
|--------|---------|
| **Status** | ğŸ”„ Planned (Task 029+) |
| **Website** | https://azure.microsoft.com/en-us/products/cognitive-services/openai-service |
| **Type** | Managed cloud service |
| **Models** | GPT-4, GPT-3.5 (Azure-hosted) |
| **API** | Azure SDK + OpenAI compatibility |
| **Lifecycle** | No process management (cloud-hosted) |
| **Operating Modes** | Burst mode only |
| **Cost Model** | Pay-per-1K tokens (with quotas) |
| **Task Assignment** | Task 029+ (Cloud Burst Orchestrators) |
| **Estimated Tasks** | 1-2 subtasks |
| **Priority** | MEDIUM (Enterprise) |

**Rationale for Selection**:
- âœ… Enterprise availability/SLA
- âœ… Azure ecosystem integration
- âœ… Regional availability
- âš ï¸ More expensive than Bedrock for same models

---

### 5. Google Vertex AI

| Aspect | Details |
|--------|---------|
| **Status** | ğŸ”„ Planned (Task 029+) |
| **Website** | https://cloud.google.com/vertex-ai |
| **Type** | Managed cloud service |
| **Models** | PaLM 2, Gemini, custom models |
| **API** | Google Cloud SDK |
| **Lifecycle** | No process management (cloud-hosted) |
| **Operating Modes** | Burst mode only |
| **Cost Model** | Pay-per-1K tokens |
| **Task Assignment** | Task 029+ (Cloud Burst Orchestrators) |
| **Estimated Tasks** | 1-2 subtasks |
| **Priority** | LOW (Smaller market share) |

**Rationale for Selection**:
- âœ… Multi-modal models (Gemini)
- âœ… Good for research/enterprise
- âš ï¸ Smaller user base

---

## Optional Future Providers (Lower Priority ğŸ’¡)

### 6. Llama.cpp

| Aspect | Details |
|--------|---------|
| **Status** | ğŸ’¡ Optional (not assigned task) |
| **Website** | https://github.com/ggerganov/llama.cpp |
| **Type** | Local binary, CPU-optimized |
| **Characteristics** | Single executable, minimal dependencies |
| **Advantages** | Lightweight, great for mobile/embedded |
| **Disadvantages** | Single quantized model at a time |
| **When to Add** | If mobile/embedded support needed |

---

### 7. LM Studio

| Aspect | Details |
|--------|---------|
| **Status** | ğŸ’¡ Optional (not assigned task) |
| **Website** | https://lmstudio.ai |
| **Type** | Local GUI application |
| **Characteristics** | User-friendly, model management UI |
| **Advantages** | No CLI knowledge needed |
| **Disadvantages** | Harder to automate than CLI |
| **When to Add** | If GUI-first users important |

---

## Excluded Providers (âŒ By Design)

### Anthropic API

| Reason for Exclusion | Details |
|-----|---|
| **Philosophy Violation** | Acode's core principle: "NO external LLM API by default" |
| **Design Intent** | LocalOnly mode should work entirely offline with Ollama/vLLM |
| **Operating Modes** | Would force Burst mode (violates LocalOnly philosophy) |
| **Alternative** | Use AWS Bedrock or Azure (which host open models, not Anthropic) |
| **Future Consideration** | Could add as explicit "Anthropic Mode" if requirements change |

**Key Point**: The system architecture intentionally prevents accidental use of external APIs. Anthropic would require explicit opt-in and architectural changes.

---

## Task Assignment Map

### Implemented Tasks

```
Task 004: Model Provider Interface (Foundation)
â”œâ”€â”€ Task 004a: Message/Tool-Call Types
â”œâ”€â”€ Task 004b: Response Format + Usage
â””â”€â”€ Task 004c: Provider Registry + Selection

Task 005: Ollama Provider Adapter
â”œâ”€â”€ Task 005a: Request/Response + Streaming âœ…
â”œâ”€â”€ Task 005c: Setup Docs + Smoke Tests âœ…
â””â”€â”€ Task 005d: Ollama Lifecycle Management âœ… (NEW)

Task 006: vLLM Provider Adapter
â”œâ”€â”€ Task 006a: Serving + Client Adapter âœ…
â”œâ”€â”€ Task 006c: Load/Health Endpoints âœ…
â””â”€â”€ Task 006d: vLLM Lifecycle Management âœ… (NEW)
```

### Missing Subtask Clarification

**Why is 005b and 006b missing?**

- **005b (Tool-call parsing)**: Moved to Task 007d (Tool Schema Registry)
  - Tool calling is cross-provider concern
  - Better to centralize in Tool Registry
  - Both Ollama and vLLM use same parsing logic

- **006b (Structured outputs)**: Assigned to Task 006c
  - Merged with health check implementation
  - vLLM API includes structured outputs
  - Not separate concern like Ollama has

### Planned Cloud Provider Tasks

```
Task 029: Cloud Burst Compute (Parent Task)
â”œâ”€â”€ Task 029a: AWS Bedrock Adapter
â”œâ”€â”€ Task 029b: Azure OpenAI Adapter
â”œâ”€â”€ Task 029c: Google Vertex AI Adapter
â”œâ”€â”€ Task 029d: Cost Tracking + Quotas
â””â”€â”€ Task 029e: Multi-region Failover
```

---

## Lifecycle Management Companion Tasks

### Pattern Recognition

**When a provider needs lifecycle management** (local service running on user's machine):
- Create `Lifecycle Management` subtask (d, e, f, etc.)
- Implement service start/stop/health/recovery
- Add configuration for operating modes
- Integrate with ProviderRegistry

### Current Lifecycle Tasks

| Provider | Adapter Task | Lifecycle Task | Status |
|----------|--------------|----------------|--------|
| Ollama | 005a | **005d** | âœ… Complete |
| vLLM | 006a | **006d** | âœ… Complete |
| Llama.cpp | TBD | TBD | ğŸ’¡ Future |
| LM Studio | TBD | TBD | ğŸ’¡ Future |
| AWS Bedrock | 029a | (N/A - cloud) | ğŸ”„ Planned |
| Azure OpenAI | 029b | (N/A - cloud) | ğŸ”„ Planned |
| Google Vertex | 029c | (N/A - cloud) | ğŸ”„ Planned |

---

## Selection Criteria

### Why These 2+3 Providers?

**Local Providers (Ollama + vLLM)**
1. âœ… Enable LocalOnly mode (no external APIs)
2. âœ… Privacy-first architecture
3. âœ… Zero infrastructure cost
4. âœ… Works offline/airgapped
5. âœ… Complementary: Ollama simple, vLLM fast

**Cloud Providers (AWS, Azure, GCP)**
1. âœ… Enable Burst mode scaling
2. âœ… Latest models without GPU
3. âœ… Enterprise SLA/support
4. âœ… Cost-effective for occasional use
5. âœ… Don't violate "no external LLM API" (they host open models)

---

## Provider Coverage Map

### By Use Case

| Use Case | Best Provider | Alternative | Avoid |
|----------|---------------|-------------|-------|
| **Hobby/Learning** | Ollama | vLLM (if GPU) | - |
| **Quick Testing** | Ollama (CPU) | - | - |
| **Performance** | vLLM | - | Ollama (slow) |
| **Enterprise** | Azure OpenAI | AWS Bedrock | - |
| **Cost-Conscious** | Ollama/vLLM | - | Cloud |
| **Privacy-Critical** | Ollama/vLLM | - | Any cloud |
| **Offline/Airgapped** | Ollama/vLLM | - | Cloud |
| **GPU-Limited** | AWS Bedrock | Azure | vLLM |

### By Operating Mode

| Mode | Supported Providers | Requirement |
|------|---------------------|-------------|
| **LocalOnly** | Ollama, vLLM | No network |
| **Burst** | Ollama, vLLM, Bedrock, Azure, GCP | Can use cloud |
| **Airgapped** | Ollama, vLLM (pre-staged) | Zero network |

---

## Why No Ollama 006b, vLLM 006b?

### Investigation Results

Reading Task 005 and 006 specifications:

**Task 005 (Ollama)**:
```
## Out of Scope
- Ollama process management
- Model downloading
- Ollama version upgrades
- Multi-Ollama routing
```

â†“ This is exactly what Task **005d** (Ollama Lifecycle) addresses!

**Task 006 (vLLM)**:
```
## Out of Scope
- vLLM installation
- GPU driver management
- Model caching strategy
```

â†“ Task **006d** (vLLM Lifecycle) handles the process, not installation/drivers.

**Missing subtasks 005b, 006b** are intentional:
- **005b (Tool calling)** â†’ Moved to Task 007d (cross-provider concern)
- **006b (Structured outputs)** â†’ Integrated into Task 006c (health check endpoint)

---

## Roadmap Summary

### Q1 2026 (Current)
- âœ… Ollama Lifecycle (005d) - COMPLETE
- âœ… vLLM Lifecycle (006d) - COMPLETE
- ğŸ”„ Ecosystem stabilization, testing

### Q2 2026 (Planned)
- ğŸ”„ AWS Bedrock (Task 029a)
- ğŸ”„ Azure OpenAI (Task 029b)
- ğŸ”„ Cost tracking (Task 029d)

### Q3 2026 (Planned)
- ğŸ”„ Google Vertex AI (Task 029c)
- ğŸ”„ Multi-region failover (Task 029e)
- ğŸ’¡ Optional local providers (Llama.cpp, LM Studio)

### Future (Backlog)
- ğŸ’¡ Anthropic (if philosophy changes)
- ğŸ’¡ Distributed inference
- ğŸ’¡ Model recommendation engine

---

## Conclusion

Acode's provider strategy:

1. **Start local**: Ollama and vLLM cover all local use cases
2. **Add cloud**: AWS, Azure, GCP for burst computing
3. **Maintain privacy**: No external LLM APIs by default
4. **Enable extensibility**: Pattern-based architecture for future providers

This balanced approach ensures users can choose: **fast & simple (Ollama)**, **GPU-optimized (vLLM)**, or **cloud scalable (AWS/Azure/GCP)** â€” all with consistent experience.

---

**End of Document**
